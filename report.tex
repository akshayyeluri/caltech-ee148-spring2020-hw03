\input{template.tex}

\title{Report for hw3 cs148}
\author{Akshay Yeluri}

\begin{document}
\maketitle{}

\subsection{Code inclusion}
You can see the code at my github repo @
https://github.com/akshayyeluri/caltech-ee148-spring2020-hw03.
In the repo, I'm also including the 2 jupyter notebooks I used
for developing my code. The first (model_building.ipynb) 
shows the process of developing the best CNN I could. The second
(analysis.ipynb) which is also attached to this report as a pdf, is the code
and the plots for questions 7 and 8 in the spec for this assignment.
The rest of the code that matters is of course in main.py.
The repo also has the models directory, which includes pytorch dumps
of the best model (best_model.pt), the original model trained without any
data augmentation (no_transform_model.pt), the original model with 
data augmentation (transform_V0.pt), and the models trained on subsets of training
data (model_sixteenth.pt, model_eighth.pt, etc). There's finally a plots
directory, including all the plots generated by analysis.ipynb.

\subsection{Results with and without augmentation}
I tried a few different transforms including cropping and normalizing. The
cropping didn't really seem to help, but the normalization did make the original
model perform better. I just normalized using 
`transforms.Normalize((0.1307,), (0.3081,))', which uses the mean and std
of the train set.

\par
Results without data augmentation (no_transform_model.pt):
Train loss=0.1690, accuracy=97.3,
Test loss=0.1614, accuracy=97.4

\par
Results with data augmentation (transform_V0.pt):
Train loss=0.0813, accuracy=97.6, 
Test loss=0.0774, accuracy=97.5

\par
What you can see is that the train and test loss both are lower with the
normalization. Also the accuracy gets better as well.

\subsection{Developing best CNN}
The process I went through to develop my architecture was essentially just a lot
of trial and error. I followed some guiding principles, like not condensing too
much in one layer, and making sure to only reduce the image size slightly in
each layer. I also tried to make sure that there was a nonlinearity and a
maxpool between each conv layer, because when I removed the maxpool layers the
performance got a lot worse. I also literally exhaustive searched to find the
probability in my dropout layer.
One trick I used a lot was testing different model's performances after one
epoch, as a proxy for how good the models would be after training. But anyway,
after a while I converged on the final model, which seems to work well.
I'm including the learning curves (the train and test loss) for the original
model without transforms, the original model with transforms, and the final
model

\begin{figure}[H]
\caption{Learning curves for original model with no transforms / data augmentation}
\centering
\includegraphics[width=0.6\textwidth]{plots/no_transform_curves.png}
\end{figure}

\begin{figure}[H]
\caption{Learning curves for original model with transforms / data augmentation}
\centering
\includegraphics[width=0.6\textwidth]{plots/transform_V0_curves.png}
\end{figure}

\begin{figure}[H]
\caption{Learning curves for final model}
\centering
\includegraphics[width=0.6\textwidth]{plots/best_model_curves.png}
\end{figure}

\subsection{Rest of the analysis}
The rest of the analysis is attached as a separate pdf generated from
analysis.ipynb.

\end{document}

